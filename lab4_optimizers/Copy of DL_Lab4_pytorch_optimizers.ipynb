{"cells":[{"cell_type":"markdown","metadata":{"id":"IzjCUmbzIPpg"},"source":["# **In this Lab, there are two exercise for evaluation, containing 5 marks each, at the end of code. Do your submissions by midnight tonight.**\n","\n","Submission Link: [Google Form](https://forms.gle/cqCNRykuQBkZb5T8A)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gJrREEhgjaKE"},"source":["# Optimizers in PyTorch\n","\n","We're going to walk through some gradient descent optimization algorithms, giving some intuition on how they work and then implementing each one in PyTorch.\n","\n","Now, let's begin by importing some libraries we'll be using."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOZCxiKzjaKG"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import tqdm"]},{"cell_type":"markdown","metadata":{"id":"MBoumFegjaKH"},"source":["We'll also set the random seed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZrAeD5bjaKI"},"outputs":[],"source":["torch.manual_seed(1234)\n","random.seed(1234)\n","np.random.seed(1234)"]},{"cell_type":"markdown","metadata":{"id":"HZx755VAjaKI"},"source":["We will be using MNIST dataset. MNIST is made up of hand drawn digits, from 0-9, represented by 28×28 pixel black-and-white images.\n","\n","We'll normalize the images using a pre-computed mean and standard deviation and perform some data augmentation, namely: randomly rotating and cropping the images.\n","\n","Note that we only get the training data as we only care about how well these optimizers minimize loss and not how well the architecture generalizes. In practice lower training loss doesn't necessarily imply better validation/test loss due to overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVUop_OcjaKP"},"outputs":[],"source":["mean = 0.13066048920154572\n","std = 0.30810779333114624\n","\n","train_transforms = transforms.Compose([\n","                            transforms.RandomRotation(5),\n","                            transforms.RandomCrop(28, padding = 2),\n","                            transforms.ToTensor(),\n","                            transforms.Normalize(mean = [mean], std = [std])])\n","\n","train_data = datasets.MNIST(root = '.data',\n","                            train = True,\n","                            download = True,\n","                            transform = train_transforms)"]},{"cell_type":"markdown","metadata":{"id":"SBljQxDsjaKP"},"source":["We then create the iterator for the data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5efFzRzsjaKP"},"outputs":[],"source":["batch_size = 128\n","\n","train_iterator = data.DataLoader(train_data,\n","                                 shuffle = True,\n","                                 batch_size = batch_size)"]},{"cell_type":"markdown","metadata":{"id":"07ClBOzmjaKQ"},"source":["Next, we'll define our architecture: a neural network with a single hidden layer.\n","\n","The `init_params` function can be called to initialize the values following the \"Kaiming\" (also known as \"He\") initialization scheme because this usually does a good job when using the ReLU activation function. The biases are initialized to zeros, which is pretty common."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7UPMLW8DjaKQ"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, input_dim, hid_dim, output_dim):\n","        super().__init__()\n","        self.layer1 = nn.Linear(input_dim, hid_dim)\n","        self.layer2 = nn.Linear(hid_dim, hid_dim)\n","        self.layer3 = nn.Linear(hid_dim, output_dim)\n","        self.init_params()\n","\n","    def init_params(self):\n","        for n, p in self.named_parameters():\n","            if 'weight' in n:\n","                nn.init.kaiming_normal_(p, nonlinearity='relu')\n","            elif 'bias' in n:\n","                nn.init.constant_(p, 0)\n","\n","    def forward(self, x):\n","        # x = [batch size, channels, height, width]\n","        batch_size, *_ = x.shape\n","        x = x.view(batch_size, -1)\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        x = self.layer3(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"FXL2w1OZjaKQ"},"source":["Our model uses a 256-dimensional hidden layer. Again, this is chosen pretty much arbitrarily and smaller values may work just as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ld-O_jtdjaKQ"},"outputs":[],"source":["input_dim = 28 * 28\n","hid_dim = 256\n","output_dim = 10\n","\n","model = MLP(input_dim, hid_dim, output_dim)"]},{"cell_type":"markdown","metadata":{"id":"pArghBaJjaKQ"},"source":["Supervised learning where each example belongs to a single class almost always uses cross-entropy loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhm3dh2gjaKQ"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"K5MMU2bXjaKR"},"source":["We'll then put the `.to` method to put the model and the loss function on to our GPU, if we have one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfUvyepujaKR"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"]},{"cell_type":"markdown","metadata":{"id":"jgfkUTVyjaKR"},"source":["Next up, we'll define some functions for training the model with our optimizers and plotting the results.\n","\n","`train_epoch` performs a single epoch of training and returns a list of losses per batch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aI2oRy33jaKR"},"outputs":[],"source":["def train_epoch(iterator, model, optimizer, criterion, device):\n","    \"\"\"Performs one epoch of training.\"\"\"\n","\n","    losses = []\n","\n","    for images, labels in tqdm.tqdm(iterator):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        predictions = model(images)\n","        loss = criterion(predictions, labels)\n","        loss.backward()\n","        optimizer.step()\n","        losses.append(loss.item())\n","\n","    return losses"]},{"cell_type":"markdown","metadata":{"id":"K6nH6A3OjaKR"},"source":["`train` initializes a model and then performs `n_epochs` of training, storing and returning the loss per batch over all the epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyU8MwnzjaKR"},"outputs":[],"source":["def train(train_iterator, model, optimizer, criterion, device, n_epochs=5):\n","    \"\"\"Trains the model for the given amount of epochs.\"\"\"\n","\n","    losses = []\n","\n","    model.init_params()\n","\n","    for epoch in range(n_epochs):\n","        epoch_losses = train_epoch(train_iterator, model, optimizer, criterion, device)\n","        losses.extend(epoch_losses)\n","\n","    return losses"]},{"cell_type":"markdown","metadata":{"id":"mXMT7mIVjaKR"},"source":["We have two functions for viewing our results.\n","\n","`plot_loss` is used for plotting the results of a single experiment. `plot_losses` plots the results of multiple experiments, which is used to compare optimizers against each other."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pssmsnMajaKR"},"outputs":[],"source":["def plot_loss(loss, title=None, ymin=0, ymax=None, figsize=(15,5)):\n","    \"\"\"Plots the loss from a single experiment.\"\"\"\n","\n","    fig, ax = plt.subplots(figsize=figsize)\n","    ax.plot(loss)\n","    ax.set_title(title)\n","    ax.set_ylabel('Loss')\n","    ax.set_xlabel('Update Steps')\n","    ax.set_ylim(ymin=ymin, ymax=ymax)\n","    ax.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPEknyYpjaKS"},"outputs":[],"source":["def plot_losses(losses, labels, title=None, ymin=0, ymax=None, figsize=(15,5)):\n","    \"\"\"Plots the losses from multiple experiments.\"\"\"\n","\n","    fig, ax = plt.subplots(figsize=figsize)\n","    for loss, label in zip(losses, labels):\n","        ax.plot(loss, label=label)\n","    ax.set_title(title)\n","    ax.set_ylabel('Loss')\n","    ax.set_xlabel('Update Steps')\n","    ax.set_ylim(ymin=ymin, ymax=ymax)\n","    ax.grid()\n","    ax.legend(loc='upper right')"]},{"cell_type":"markdown","metadata":{"id":"Se-xwhs4jaKS"},"source":["Now let's implement our first optimizer!\n","\n","## Optimizer 1: Stochastic Gradient Descent (SGD)\n","\n","Stochastic gradient descent is the simplest optimization algorithm, so it's a good place to start. We take our current model parameters $\\theta_t$ and subtract the gradient of those parameters, $\\nabla_\\theta J(\\theta_t)$, multiplied by the \"learning rate\", $\\eta$.\n","\n","We can think of the learning rate as a parameter that controls the magnitude of the parameter update. If our learning rate is too small then our parameter updates will also be too small for us to train our model in a reasonable amount of time. Conversely, if our learning rate is too large then the size of the parameter updates will be so large that learning will become unstable! If you ever get a `NaN` value for your loss, one of the first things to try would be lowering the learning rate.\n","\n","The SGD algorithm is:\n","\n","$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta J(\\theta_t)$$\n","\n","However, we don't just have one set of parameters, $\\theta$, we have multiple parameters: the weights of layer 1, the biases of layer 1, the weights of layer 2, the biases of layer 2, etc. So we'll subscript the parameters with $i$:\n","\n","$$\\theta_{t+1,i} = \\theta_{t,i} - \\eta \\cdot \\nabla_\\theta J(\\theta_{t,i})$$\n","\n","We subtract because we want to descend the gradient and move towards a lower loss value. Addition would ascend the gradient, hence it's called gradient ascent.\n","\n","One final thing to mention is the difference between gradient descent, stochastic gradient descent, mini-batch gradient descent and on-line gradient descent. **Gradient descent** means we calculate the gradient using every single example in our training set and then do a single parameter update. This is relatively slow as in our experiments it means only updating the parameters after seeing all 60,000 examples. The other extreme is **stochastic gradient descent** which means we update our parameters after every single example. This is usually very noisy, so a happy medium is updating the parameters after we have seen a *mini-batch* of examples,  **mini-batch gradient descent**. Lastly, **on-line gradient descent**, which usually implies our model is in production and is being constantly fed new examples on which it is using to update its parameters.\n","\n","**Gradient descent** is sometimes called **batch gradient descent**, where the whole dataset counts as one giant batch, hence using sampled batch of examples is called a *mini-batch*.\n","\n","In PyTorch, the optimizer is called *stochastic gradient descent* even though it can do any of the above gradient descent variants. The general rule of thumb is that nowadays when someone mentions stochastic gradient descent then they mean mini-batch gradient descent.\n","\n","Moving on to the implementation. All optimizers need a way of keeping track of the parameters they're supposed to be updating `model_params` and a learning rate, `lr`. SGD in PyTorch doesn't have a default learning rate but `1e-3` is a common default learning rate value for other optimizers, so we use it here. All optimizers need a `zero_grad` function in order to remove the gradients calculated from the last update step, and a `step` function to perform a parameter update.\n","\n","Note that any PyTorch method with a trailing underscore, e.g., `.sub_`, means the operation is in-place. This means our `step` function is updating each `param`, a tensor of parameters, in-place. These in-place operations are usually significantly faster non in-place operations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAPX9RmzjaKS"},"outputs":[],"source":["class SGD:\n","    def __init__(self, model_params, lr=1e-3):\n","        self.model_params = list(model_params)\n","        self.lr = lr\n","\n","    def zero_grad(self):\n","        for param in self.model_params:\n","            param.grad = None\n","\n","    @torch.no_grad()\n","    def step(self):\n","        for param in self.model_params:\n","            param.sub_(self.lr * param.grad)"]},{"cell_type":"markdown","metadata":{"id":"G7TqUwwhjaKS"},"source":["We can define our optimizer like so:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsvR_lO4jaKS"},"outputs":[],"source":["optimizer = SGD(model.parameters())"]},{"cell_type":"markdown","metadata":{"id":"KbfuFM_FjaKS"},"source":["Then we use it to train our model for five epochs and get the training loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b44VX852jaKS"},"outputs":[],"source":["sgd_loss = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"markdown","metadata":{"id":"o9vk8fCOjaKT"},"source":["Now let's plot it and see what it looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCnZCB7sjaKT"},"outputs":[],"source":["plot_loss(sgd_loss, 'SGD with lr=1e-3')"]},{"cell_type":"markdown","metadata":{"id":"ygFIotU3jaKT"},"source":["Looks reasonable, the loss starts at a high value as our parameters are randomly initialized and then proceeds to decrease steadily. We can't really tell how \"good\" it is without comparing it against another optimizer, so let's go ahead and do that now.\n","\n","## Optimizer 2: SGD with Momentum\n","\n","One way to think of SGD is a ball rolling down a hill, where areas of high gradient are steep parts of the hill and areas of low gradient are very flat areas. Sometimes the global minima, the point with the lowest loss, is in the middle of a giant flat area. The problem is that because these flat areas have small gradients they also give small update steps which makes learning slow.\n","\n","We'd want to add something to our optimizer that made it keep the \"momentum\" gained rolling down the steep hills whilst it's going across the flat areas.\n","\n","That's exact what SGD with momentum does! Our parameter update is now calculated using a velocity, $v$, which depends on the current gradient multiplied by the learning rate plus the previous velocity multiplied by the momentum $\\gamma$.\n","\n","\\begin{align*}\n","    v_{t,i} &= \\gamma \\cdot v_{t-1,i} + \\eta \\cdot \\nabla_\\theta J(\\theta_{t,i})\\\\\n","    \\theta_{t+1,i} &= \\theta_{t,i} - v_{t,i}\\\\\n","\\end{align*}\n","\n","If momentum is zero then we don't care about the previous velocity at all and this algorithm becomes SGD. Commonly used momentum values are usually around 0.9.\n","\n","PyTorch's optimizers are sometimes a little different from the actual algorithms. PyTorch's version of SGD with momentum moves the learning rate outside the equation for velocity:\n","\n","\\begin{align*}\n","    v_{t,i} &= \\gamma \\cdot v_{t-1,i} + \\nabla_\\theta J(\\theta_{t,i})\\\\\n","    \\theta_{t+1,i} &= \\theta_{t,i} - \\eta \\cdot v_{t,i}\\\\\n","\\end{align*}\n","\n","If the PyTorch implementation differs then we'll implement the PyTorch version as we use it as a reference.\n","\n","Note that the velocity `v` is a list of tensors corresponding to the model parameters, so we are storing the velocity of every single parameter in our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIACWuhOjaKT"},"outputs":[],"source":["class SGDMomentum:\n","    def __init__(self, model_params, lr=1e-3, momentum=0.9):\n","        self.model_params = list(model_params)\n","        self.lr = lr\n","        self.momentum = momentum\n","        self.v = [torch.zeros_like(p) for p in self.model_params]\n","\n","    def zero_grad(self):\n","        for param in self.model_params:\n","            param.grad = None\n","\n","    @torch.no_grad()\n","    def step(self):\n","        for param, v in zip(self.model_params, self.v):\n","            v.mul_(self.momentum).add_(param.grad)\n","            param.sub_(self.lr * v)"]},{"cell_type":"markdown","metadata":{"id":"XAqHyZJejaKU"},"source":["Below we define our SGD with momentum optimizer, train our model and plot the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1M7NgfWjaKU"},"outputs":[],"source":["optimizer = SGDMomentum(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_N0I-YdZjaKU"},"outputs":[],"source":["sgd_momentum_loss = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1Xz38eQjaKU"},"outputs":[],"source":["plot_loss(sgd_momentum_loss, 'SGDMomentum with lr=1e-3, momentum=0.9')"]},{"cell_type":"markdown","metadata":{"id":"YL0shR0MjaKU"},"source":["Now let's compare SGD and SGD with momentum."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l--ScWuLjaKU"},"outputs":[],"source":["losses = [sgd_loss, sgd_momentum_loss]\n","labels = ['sgd', 'sgd_momentum']\n","\n","plot_losses(losses, labels, 'SGD vs SGDMomentum')"]},{"cell_type":"markdown","metadata":{"id":"RFt07UhYjaKU"},"source":["As we can see, not only does momentum help us reach the lowest SGD loss, around 1.0, in a fraction of the time it also gives us a lower overall!\n","\n","Let's move on to the next optimizer.\n","\n","## Optimizer 3: Adagrad (Optional)\n","\n","One downside with SGD is that we use a single learning rate across all of our parameters, and that this learning rate is fixed through the entirety of training.\n","\n","Ideally, parameters that are updated more frequently have a lower learning rate and parameters that are updated infrequently have a larger learning rate.\n","\n","This is what Adagrad does. We use $G_{t,i}$ which is the sum of the squared gradients for parameter $i$ up to, and including, time-step $t$. $G_{t,i}$ is initialized to some value, usually zero by default. As the square of the gradients of a parameter are accumulated, $G_{t,i}$ increases, and thus reduces the learning rate for parameter $i$.\n","\n","$$\\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\eta}{\\sqrt{G_{t,i}}+\\epsilon} \\cdot \\nabla_\\theta J(\\theta_{t,i})$$\n","\n","where:\n","\n","$$G_{t,i} = G_{t-1,i} + \\Big(\\nabla_\\theta J(\\theta_{t,i})\\Big)^2$$\n","\n","$\\epsilon$ is very small number, used to avoid division by zero in the denominator. Sometimes you'll see $\\epsilon$ inside the square root, and sometimes it will be outside. PyTorch leaves it outside so we will too.\n","\n","We implement Adagrad below, initializing $G$ as a list of tensors called `acc_sqr_grads` and using `std` to refer to the denominator of the update step equation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fj6Zghg5jaKV"},"outputs":[],"source":["class Adagrad:\n","    def __init__(self, model_params, lr=1e-2, init_acc_sqr_grad=0, eps=1e-10):\n","        self.model_params = list(model_params)\n","        self.lr = lr\n","        self.acc_sqr_grads = [torch.full_like(p, init_acc_sqr_grad) for p in self.model_params]\n","        self.eps = eps\n","\n","    def zero_grad(self):\n","        for param in self.model_params:\n","            param.grad = None\n","\n","    @torch.no_grad()\n","    def step(self):\n","        for param, acc_sqr_grad in zip(self.model_params, self.acc_sqr_grads):\n","            acc_sqr_grad.add_(param.grad * param.grad)\n","            std = acc_sqr_grad.sqrt().add(self.eps)\n","            param.sub_((self.lr / std) * param.grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqGRydRPjaKV"},"outputs":[],"source":["optimizer = Adagrad(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlFVPmmujaKV","scrolled":true},"outputs":[],"source":["adagrad_loss = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TyYlrEW5jaKV"},"outputs":[],"source":["plot_loss(adagrad_loss, 'Adagrad with lr=1e-2, init_acc_sqr_grad=0, eps=1e-10')"]},{"cell_type":"markdown","metadata":{"id":"Q7B1UBHrjaKV"},"source":["We can see there's an initial large spike in the loss value. This is due to the initial $G$ values being very small and thus the learning rate is divided by a very small number making it very large. Very large learning rates usually lead to unstable training which give higher loss values.\n","\n","Let's trim the start to get a better view what the final loss value is."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avyLMinqjaKV"},"outputs":[],"source":["plot_loss(adagrad_loss, 'Adagrad with lr=1e-2, init_acc_sqr_grad=0, eps=1e-10', ymax=5.0)"]},{"cell_type":"markdown","metadata":{"id":"5YkJNvvrjaKW"},"source":["Let's compare our Adagrad loss to our SGD algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BeqBc5njaKW"},"outputs":[],"source":["losses = [sgd_loss, sgd_momentum_loss, adagrad_loss]\n","labels = ['sgd', 'sgd_momentum', 'adagrad_loss']\n","\n","plot_losses(losses, labels, 'SGD vs. SGDMomentum vs. Adagrad')"]},{"cell_type":"markdown","metadata":{"id":"gDkF_GnQjaKX"},"source":["Adagrad beats the other two pretty handily, but that initial spike in loss doesn't look very nice. Maybe if we get rid of that initial spike we can make Adagrad perform even better? Let's try some different initial values for $G$ and store them all in a `adagrad_losses` dictionary. Each key in the dictionary will be the initial $G$ value and the values of the dictionary will be a list of training loss per batch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJXQ_-5yjaKX"},"outputs":[],"source":["adagrad_losses = {0: adagrad_loss}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kAvd-H3ujaKX"},"outputs":[],"source":["optimizer = Adagrad(model.parameters(), init_acc_sqr_grad=1.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CrSIu4eojaKY"},"outputs":[],"source":["adagrad_losses[1.0] = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Iq4SihjCjaKY"},"outputs":[],"source":["optimizer = Adagrad(model.parameters(), init_acc_sqr_grad=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oaGHXHpPjaKY"},"outputs":[],"source":["adagrad_losses[0.1] = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"__wo1GXNjaKY"},"outputs":[],"source":["optimizer = Adagrad(model.parameters(), init_acc_sqr_grad=0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rMHtJUZGjaKY"},"outputs":[],"source":["adagrad_losses[0.01] = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yoC5P1W1jaKZ"},"outputs":[],"source":["optimizer = Adagrad(model.parameters(), init_acc_sqr_grad=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aDJ2oci4jaKZ"},"outputs":[],"source":["adagrad_losses[0.001] = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"markdown","metadata":{"id":"kzvlq1pOjaKZ"},"source":["Now let's compare all of our four values for the initial $G$ value."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ci26eID0jaKZ"},"outputs":[],"source":["labels, losses = zip(*adagrad_losses.items())\n","\n","plot_losses(losses, labels, 'Adagrad init_acc_sqr_grad Value Comparison', ymax=5.0)"]},{"cell_type":"markdown","metadata":{"id":"V_zZM8UIjaKZ"},"source":["As we can see, performance of Adagrad increases as the initial $G$ value decreases, but decreasing $G$ also increases the initial spike in loss at the beginning of training.\n","\n","Why does the performance decrease as the initial $G$ value increases? This is the major downside of Adagrad: as $G$ is monotonically increasing at each time-step it will be dividing the learning rate by a monotonically increasing number at each time-step. This causes the size of the steps taken to reduce every update step. As the results for an initial $G$ value of 1.0 show, we can see that these smaller step sizes actually increase the time taken for the model to converge, and in extreme cases will cause the step sizes to approach zero meaning the parameters will stop updating completely.\n","\n","In practice, we do want the learning rate to decrease whilst training, but ideally would not want it to become zero.\n","\n","Let's compare Adagrad against the SGD optimizers with the large initial spike trimmed off, so we can get a better view of how they compare."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GevVdLK-jaKZ"},"outputs":[],"source":["losses = [sgd_loss, sgd_momentum_loss, adagrad_loss]\n","labels = ['sgd', 'sgd momentum', 'adagrad']\n","\n","plot_losses(losses, labels, 'SGD Optimizers vs. Adagrad', ymax=5.0)"]},{"cell_type":"markdown","metadata":{"id":"Go5rJb3vjaKZ"},"source":["## Optimizer 4: Adadelta (Optional)\n","\n","All of our update step equations can be written in the form of:\n","\n","$$\\theta_{t+1,i} = \\theta_{t,i} + \\Delta \\theta_{t,i}$$\n","\n","where $\\Delta \\theta_{t,i}$ is the size of the parameter update, i.e. in SGD we had:\n","\n","$$\\Delta \\theta_{t,i} = - \\eta \\cdot \\nabla_\\theta J(\\theta_{t,i})$$\n","\n","and in Adagrad we had:\n","\n","$$\\Delta \\theta_{t,i} = - \\frac{\\eta}{\\sqrt{G_{t,i}}+\\epsilon} \\cdot \\nabla_\\theta J(\\theta_{t,i}))$$\n","\n","The problem of the Adagrad algorithm was that $G$ was monotonically increasing. Adadelta solves this problem by first taking the Adagrad algorithm and replacing $G_{t,i}$ with $E[g^2]_{t,i}$, an exponential moving average of the square of the gradients so far.\n","\n","$$E[g^2]_{t,i} = \\rho E[g^2]_{t-1,i} + (1-\\rho)g^2_{t,i}$$\n","\n","where $g_{t,i} = \\nabla_\\theta J(\\theta_{t,i})$, which we've done just to simplify the notation, and $\\rho$ controls how much we care about the previous gradients in the exponential moving average, $\\rho=0$ means we don't care about them at all.\n","\n","This means our update step equation is:\n","\n","$$\\Delta \\theta_t = - \\frac{\\eta}{\\sqrt{E[g^2]_{t,i} + \\epsilon}} \\cdot g_{t,i}$$\n","\n","Notice that the $\\epsilon$ term has now moved inside the square root, which we're copying from PyTorch.\n","\n","The problem with the above equation, and in fact all update equations seen so far, is the units of the update do not match the units of the parameters. The updates have units of $\\frac{\\delta J}{\\delta \\theta}$, which simplify to $\\frac{1}{\\text{units of }\\theta}$ if we assume the cost function is unitless. However, we want our update equations to have units of $\\theta$.\n","\n","To solve this the Adadelta equation uses a second exponential moving average, but this one is of the parameter updates.\n","\n","To get the final Adadelta equation, we take our first attempt, but replace $\\eta$ with an exponential moving average of the squared parameter updates:\n","\n","$$E[\\Delta \\theta^2]_{t-1,i} = \\rho E[\\Delta \\theta^2]_{t-2,i} + (1-\\rho)\\Delta \\theta^2_{t-1,i}$$\n","\n","Thus, we get:\n","\n","$$\\Delta \\theta_{t,i} = - \\frac{\\sqrt{E[\\Delta \\theta^2]_{t-1,i} + \\epsilon}}{\\sqrt{E[g^2]_{t,i}+\\epsilon}} \\cdot g_{t,i}$$\n","\n","The units now \"match\" as we now have units of $\\frac{\\theta^2}{\\theta} = \\theta$.\n","\n","This means that we do not even need to use a learning rate value, however in the PyTorch implementation they do use one (which defaults to 1.0), so they end up with:\n","\n","$$\\Delta \\theta_{t,i} = - \\eta \\cdot \\frac{\\sqrt{E[\\Delta \\theta^2]_{t-1,i} + \\epsilon}}{\\sqrt{E[g^2]_{t,i}+\\epsilon}} \\cdot g_{t,i}$$\n","\n","Thus:\n","\n","$$\\theta_{t+1,i} = \\theta_{t,i} - \\eta \\cdot \\frac{\\sqrt{E[\\Delta \\theta^2]_{t-1,i} + \\epsilon}}{\\sqrt{E[g^2]_{t,i}+\\epsilon}} \\cdot g_{t,i}$$\n","\n","PyTorch also changes default `eps` value from what it was in Adagrad, from `1e-10` to `1e-6`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dDdU6hhAjaKa"},"outputs":[],"source":["class Adadelta:\n","    def __init__(self, model_params, lr=1.0, rho=0.9, eps=1e-6):\n","        self.model_params = list(model_params)\n","        self.lr = lr\n","        self.rho = rho\n","        self.eps = eps\n","        self.avg_sqr_grads = [torch.zeros_like(p) for p in self.model_params]\n","        self.avg_sqr_deltas = [torch.zeros_like(p) for p in self.model_params]\n","\n","    def zero_grad(self):\n","        for param in self.model_params:\n","            param.grad = None\n","\n","    @torch.no_grad()\n","    def step(self):\n","        for param, avg_sqr_grad, avg_sqr_delta in zip(self.model_params, \\\n","                                                      self.avg_sqr_grads, \\\n","                                                      self.avg_sqr_deltas):\n","            avg_sqr_grad.mul_(self.rho).add_(param.grad * param.grad * (1 - self.rho))\n","            std = avg_sqr_grad.add(self.eps).sqrt()\n","            delta = avg_sqr_delta.add(self.eps).sqrt().div(std).mul(param.grad)\n","            param.sub_(self.lr * delta)\n","            avg_sqr_delta.mul_(self.rho).add_(delta * delta * (1 - self.rho))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_SSAR233jaKa"},"outputs":[],"source":["optimizer = Adadelta(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WVUB-OhPjaKa","scrolled":true},"outputs":[],"source":["adadelta_loss = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sbx7MnWYjaKa"},"outputs":[],"source":["plot_loss(adadelta_loss, 'Adadelta with lr=1.0, rho=0.9, eps=1e-6')"]},{"cell_type":"markdown","metadata":{"id":"Kx0W1XBQjaKa"},"source":["We can see that we avoid the large initial spike in loss due to the numerator term (the exponential moving average of parameter updates) starting out very small.\n","\n","Let's compare Adadelta to all the other algorithms so far."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YPa0Pak3jaKa"},"outputs":[],"source":["losses = [sgd_loss, sgd_momentum_loss, adagrad_loss, adadelta_loss]\n","labels = ['sgd', 'sgd momentum', 'adagrad', 'adadelta']\n","\n","plot_losses(losses, labels, 'SGD vs. SGD Momentum vs. Adagrad vs. Adadelta', ymax=5.0)"]},{"cell_type":"markdown","metadata":{"id":"CXU35HQBjaKb"},"source":["We can see that Adagrad and Adadelta have pretty much equal performance, but Adadelta doesn't have the large initial spike in loss.\n","\n","## Optimizer 5: RMSprop\n","\n","$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_{t,i} + \\epsilon}} \\cdot g_{t,i}$$\n","\n","In PyTorch, they move the $\\epsilon$ term back outside of the square root. This gives us:\n","\n","$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_{t,i}} + \\epsilon} \\cdot g_{t,i}$$\n","\n","In the PyTorch implementation they also change the default learning rate to `1e-2`, rename `rho` to `alpha` whilst giving it a new default value of `0.99`, and also change the default `eps` to `1e-8`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DUN0v755jaKb"},"outputs":[],"source":["class RMSprop:\n","    def __init__(self, model_params, lr=1e-2, alpha=0.99, eps=1e-8):\n","        self.model_params = list(model_params)\n","        self.lr = lr\n","        self.alpha = alpha\n","        self.eps = eps\n","        self.avg_sqr_grads = [torch.zeros_like(p) for p in self.model_params]\n","\n","    def zero_grad(self):\n","        for param in self.model_params:\n","            param.grad = None\n","\n","    @torch.no_grad()\n","    def step(self):\n","        for param, avg_sqr_grad in zip(self.model_params, self.avg_sqr_grads):\n","            avg_sqr_grad.mul_(self.alpha).add_(param.grad * param.grad * (1 - self.alpha))\n","            std = avg_sqr_grad.sqrt().add(self.eps)\n","            param.sub_((self.lr / std) * param.grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0CUmN_odjaKb"},"outputs":[],"source":["optimizer = RMSprop(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hFP9l-7DjaKb"},"outputs":[],"source":["rmsprop_loss = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TkUAkojCjaKb"},"outputs":[],"source":["plot_loss(rmsprop_loss, 'RMSprop with lr=1e-2, alpha=0.99, eps=1e-8')"]},{"cell_type":"markdown","metadata":{"id":"0WYXN5k3jaKb"},"source":["We run into a similar issue as we did with Adagrad, the small denominator in the initial time-steps lead to large step sizes which give huge spikes in loss values during the early stages of training.\n","\n","Let's zoom in to get a better view of what's going on."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wWj1mYcQjaKc"},"outputs":[],"source":["plot_loss(rmsprop_loss, 'RMSprop with lr=1e-2, alpha=0.99, eps=1e-8', ymax=5.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iMN5xQi4jaKd"},"outputs":[],"source":["rmsprop_losses = {1e-8: rmsprop_loss}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NwLXRy20jaKd"},"outputs":[],"source":["optimizer = RMSprop(model.parameters(), eps=1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CRYIfaA1jaKd"},"outputs":[],"source":["rmsprop_losses[1e-6] = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sw7YV-NHjaKe"},"outputs":[],"source":["optimizer = RMSprop(model.parameters(), eps=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BD4fKsKkjaKe"},"outputs":[],"source":["rmsprop_losses[1e-4] = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ppyp_n0ljaKe"},"outputs":[],"source":["optimizer = RMSprop(model.parameters(), eps=1e-2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vGzGDE4njaKe"},"outputs":[],"source":["rmsprop_losses[1e-2] = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hVy9gksBjaKe"},"outputs":[],"source":["optimizer = RMSprop(model.parameters(), eps=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BapKSQotjaKe","outputId":"3be22858-d239-4719-ae96-b8f9197947ba","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 469/469 [00:29<00:00, 16.06it/s]\n"," 41%|████▏     | 194/469 [00:12<00:21, 12.60it/s]"]}],"source":["rmsprop_losses[1] = train(train_iterator, model, optimizer, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwN0CqDEjaKf"},"outputs":[],"source":["labels, losses = zip(*rmsprop_losses.items())\n","\n","plot_losses(losses, labels, 'RMSprop eps Value Comparison', ymax=5.0)"]},{"cell_type":"markdown","metadata":{"id":"amQ0VuktjaKf"},"source":["Increasing `eps` improves performance to a point, `eps=1e-2` gives the best performance, and then performance starts degrading, `eps=1` gives the worst performance."]},{"cell_type":"markdown","metadata":{"id":"uakJFbt2jaKh"},"source":["Let's compare RMSprop against Adagrad and Adadelta."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCrUN6wBjaKh"},"outputs":[],"source":["losses = [adagrad_loss, adadelta_loss, rmsprop_losses[1e-2]]\n","labels = ['adagrad', 'adadelta', 'rmsprop']\n","\n","plot_losses(losses, labels, 'Adagrad vs. Adadelta vs. RMSprop', ymax=5.0)"]},{"cell_type":"markdown","metadata":{"id":"o4CCDDuvjaKh"},"source":["They're all very close, maybe RMSprop is slightly worse?\n","\n","Let's zoom in."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aL7ygEr2jaKh"},"outputs":[],"source":["losses = [adagrad_loss, adadelta_loss, rmsprop_losses[1e-2]]\n","labels = ['adagrad', 'adadelta', 'rmsprop']\n","\n","plot_losses(losses, labels, 'Adagrad vs. Adadelta vs. RMSprop', ymax=1.0)"]},{"cell_type":"markdown","metadata":{"id":"TsHNxwhljaKh"},"source":["It's still a bit hard to tell.\n","\n","How about we smooth the loss curves with a moving average?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9La-0eIjaKh"},"outputs":[],"source":["def moving_average(x, w=5):\n","    return np.convolve(x, np.ones(w), 'valid') / w"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H_T7ycSFjaKh"},"outputs":[],"source":["losses = [adagrad_loss, adadelta_loss, rmsprop_losses[1e-2]]\n","smoothed_losses = [moving_average(loss) for loss in losses]\n","labels = ['adagrad', 'adadelta', 'rmsprop']\n","\n","plot_losses(smoothed_losses, labels, 'Adagrad vs. Adadelta vs. RMSprop', ymax=1.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8PZfv03l__T"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lShWt7ceuUzH"},"source":["#Exercises"]},{"cell_type":"markdown","metadata":{"id":"oEQVXFDd1YCc"},"source":["**Excercise 1:**\n","\n","\n","\n","You have to implement mini-batch training for this code.\n","The losses need to be added up for n iterations before a backprop steps needs to take place. This can be done in multiple ways; it is left to you to decide how you would like to implement this\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYsjANF_-r-N"},"outputs":[],"source":["\n","def train_epoch(iterator, model, optimizer, criterion, device, accumulation_steps):\n","\n","    model.train()\n","    losses = []\n","    total_loss = 0.0\n","\n","    for i, (images, labels) in enumerate(tqdm.tqdm(iterator, leave=False)):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        predictions = model(images)\n","        loss = criterion(predictions, labels)\n","\n","        loss.backward()\n","\n","        if (i + 1) % accumulation_steps == 0:\n","            optimizer.step()\n","            losses.append(total_loss / accumulation_steps)\n","            total_loss = 0.0\n","        else:\n","            total_loss += loss.item()\n","\n","    if total_loss > 0:\n","        optimizer.step()\n","        losses.append(total_loss / (i % accumulation_steps + 1))\n","\n","    return losses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WpgMp6tf-9ex"},"outputs":[],"source":["def train(train_iterator, model, optimizer, criterion, device, n_epochs=5, accumulation_steps=1):\n","    losses = []\n","\n","    model.init_params()\n","\n","    for epoch in range(n_epochs):\n","        epoch_losses = train_epoch(train_iterator, model, optimizer, criterion, device, accumulation_steps)\n","        losses.extend(epoch_losses)\n","\n","    return losses\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHACp3OY-_wl"},"outputs":[],"source":["num_epochs = 5\n","accumulation_steps = 1  # set as 1 for mini batch training\n","\n","optimizer = optim.Adam(model.parameters())\n","train_losses = train(train_iterator, model, optimizer, criterion, device, n_epochs=num_epochs, accumulation_steps=accumulation_steps)"]},{"cell_type":"markdown","metadata":{"id":"7ZE3BagujaKi"},"source":["**Exercise 2:**\n","\n","Adam has an exponential moving average of the gradients, like the momentum term that can be added to SGD, and an exponential moving average of squared gradients, like RMSprop.\n","\n","$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{m_{t,i}}{\\sqrt{v_{t,i}}+\\epsilon}$$\n","\n","where:\n","\n","\\begin{align*}\n","    m_{t,i} &= \\beta_1 m_{t-1,i} + (1-\\beta_1)g_{t,i} \\\\\n","    v_{t,i} &= \\beta_2 v_{t-1,i} + (1-\\beta_2)g_{t,i}^2\n","\\end{align*}\n","\n","Adam's $m_{t,i}$ is equal to $v_{t,i}$ from SGD with momentum if it had a $(1-\\gamma)$ term. Adam's $v_{t,i} = E[g^2]_{t,i}$ from RMSprop, with $\\rho$ replaced by $\\beta_2$.\n","\n","As $m$ and $v$ are initialized to zero and $\\beta_1$ and $\\beta_2$ are initialized close to one the $m$ and $v$ values calculated on the first few update steps are \"biased\" towards very small values. This is why we saw a huge loss for the first steps of Adagrad, Adadelta and RMSprop.\n","\n","To solve this, Adam uses \"bias corrected\" values of $m$ and $v$, calculated as:\n","\n","\\begin{align*}\n","    \\hat{m}_{t,i} &= \\frac{m_{t,i}}{1-\\beta_1^t} \\\\\n","    \\hat{v}_{t,i} &= \\frac{v_{t,i}}{1-\\beta_2^t}\n","\\end{align*}\n","\n","This gives the final Adam equation as:\n","\n","$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{\\hat{m}_{t,i}}{\\sqrt{\\hat{v}_{t,i}}+\\epsilon}$$\n","\n","Note that the bias corrected values on the first call to `step` are calculated with $t = 1$ and not $t = 0$."]},{"cell_type":"markdown","metadata":{"id":"qrK0uEjBtK4k"},"source":["**Complete the below code.**(Areas with # sign)\n","\n","**Plot the comparison curves** for given code of **ADAM** against **SGD with momentum** and **RMSprop**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ooYpu2u2wor"},"outputs":[],"source":["class Adam:\n","    def __init__(self, model_params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n","        self.model_params = model_params\n","        self.lr = lr\n","        self.beta_1, self.beta_2 = betas\n","        self.eps = eps\n","        self.avg_grads = [torch.zeros_like(param) for param in model_params]\n","        self.avg_sqr_grads = [torch.zeros_like(param) for param in model_params]\n","        self.n_steps = 0\n","\n","    def zero_grad(self):\n","        for param in self.model_params:\n","            param.grad = torch.zeros_like(param)\n","\n","    @torch.no_grad()\n","    def step(self):\n","        self.n_steps += 1\n","        for i, (param, avg_grad, avg_sqr_grad) in enumerate(zip(self.model_params, self.avg_grads, self.avg_sqr_grads)):\n","            grad = param.grad\n","\n","            avg_grad.mul_(self.beta_1).add_(grad*grad*(1 - self.beta_1))\n","            avg_sqr_grad.mul_(self.beta_2).add_(grad* grad*(1-self.beta_2))\n","\n","            avg_grad_corrected = avg_grad.div(1 - self.beta_1 ** self.n_steps)\n","            avg_sqr_grad_corrected = avg_sqr_grad.div(1 - self.beta_2 ** self.n_steps)\n","            std = avg_sqr_grad_corrected.sqrt().add(self.eps)\n","            param.sub_(self.lr * avg_grad_corrected / std)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKDEP8X-yc9R"},"outputs":[],"source":["optimizer = Adam(model.parameters())\n","optimizer2 = optim.Adam(model.parameters() , lr = 1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2IOBz7Xykey"},"outputs":[],"source":["adam_loss = train(train_iterator, model, optimizer2, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXNgiYJUzGGs"},"outputs":[],"source":["plot_loss(adam_loss, 'Adam with lr=1e-3, betas = (0.9 , 0.999), eps=1e-8')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUVtESfr1fgy"},"outputs":[],"source":["losses = [adam_loss , sgd_momentum_loss , rmsprop_loss]\n","labels = [\"adam\" , \"sgd_momentum\" , \"rms_prop\"]\n","plot_losses(losses, labels, 'RMSprop eps Value Comparison', ymax=5.0)"]},{"cell_type":"markdown","metadata":{"id":"Ek-_1p858Mmg"},"source":["#References\n","https://ruder.io/optimizing-gradient-descent/\n","\n","https://mlfromscratch.com/optimizers-explained/#/\n","\n","https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/\n","\n","https://pytorch.org/docs/stable/optim.html\n","\n","https://github.com/pytorch/pytorch/tree/master/torch/optim\n","\n","https://www.coursera.org/learn/machine-learning/"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ygFIotU3jaKT","RFt07UhYjaKU","Go5rJb3vjaKZ","CXU35HQBjaKb"],"provenance":[{"file_id":"1QO_AEAawEucG4JQUvwhR8qP_O3FxnOLZ","timestamp":1694377716256},{"file_id":"1PM5zrHC9OM7aUN1tGY6F2A94rn8ketSm","timestamp":1694267147259}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}